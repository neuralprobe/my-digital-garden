---
title: Multiprocessors and Thread-Level Parallelism
date: 2023-05-25
tags: ComputerArchitecture ThreadLevelParallelism Multiprocessing HennessyPatterson
---

# Multiprocessors and Thread-Level Parallelism

- Chapter 5 in [Computer Architecture A Quantitative Approach (6th)](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1) by Hennessy and Patterson (2017)

---

# Introduction

The **increased importance of multiprocessing** reflects several major factors:
- Inefficiency of adding more instruction-level parallelism?
	- $\rightarrow$ **Multiprocessing** = Only scalable and general-purpose way 
- A growing interest in **high-end servers** as **cloud computing** and s**oftware-as-a-service**
- A growth in **data-intensive** applications
- Increasing performance on the **desktop is less important** 
	- Highly compute- and data-intensive applications are being done on the **cloud**
- An improved **understanding** of how to use multiprocessors effectively
- The advantages of leveraging a design **investment by replication** rather than unique design

---

In this chapter, we focus on exploiting **thread-level parallelism (TLP)**. **TLP** implies the existence of **multiple program counters** and thus is exploited primarily through **MIMDs**. 

Our focus in this chapter is on **multiprocessors**, which we define as **computers consisting of tightly coupled processors**: 
- Whose coordination and usage are typically controlled by a **single operating system** and 
- That **share memory through a shared address space**.

Such systems exploit thread-level parallelism through **two different software models**:
1. The execution of a **tightly coupled set of threads** collaborating on a **single task**, which is typically called **parallel processing**
2. The execution of multiple, relatively **independent processes** that may originate from one or more users, which is a form of **request-level parallelism** (See Chapter 6 [[Warehouse-Scale Computer]])
	- Maybe exploited **by a single application** running on multiple processors, such as a **database responding to queries**, 
	- Or multiple applications running independently, often called **multiprogramming**.

**Multithreading?** 
- A technique that supports multiple threads executing
- In an interleaved fashion on a single multiple-issue processor
- Many multicore processors also include support for multithreading

Our focus will be on multiprocessors with **roughly 4–256 processor cores**, which might occupy anywhere from **4 to 16 separate chips**;
- **Multicore**?: Multiprocessors in a single chip

**Larger scale computers?**
- See Chapter 6 [[Warehouse-Scale Computer]]
- **Clusters**: Large-scale systems for cloud computing 
- **Warehouse-scale computers**: Extremely large clusters
- **Multicomputer**: 
	- Less tightly coupled than the multiprocessors
	- More tightly coupled than warehouse-scale systems
	- Primary use for high-end scientific computation

---

## Multiprocessor Architecture: Issues and Approach

To take advantage of an **MIMD multiprocessor with $n$ processors**, we must usually have **at least $n$ threads or processes** to execute; 
- With **multithreading**, that **number is 2–4 times higher**.

The **independent threads** within a single process are:
- Identified by the **programmer** or
- Created by the **operating system** (from multiple independent requests) or
- Generated by a **parallel compiler** exploiting data parallelism in the loop

**Grain size:**
- The amount of computation assigned to a thread

**Thread for data-level parallelism**?
- Possible, but, with higher overhead than SIMD processor or GPU due to its small grain size

**Two classes** of shared-memory multiprocessors (**SMP vs DSM architectures** ):
1. **Symmetric (shared-memory) multiprocessors (SMPs)** in Figure 5.1
	- Centralized shared-memory multiprocessors
	- Small to moderate number of cores $\leq 32$
	- The processors share a single centralized memory
	- All processors have equal access $\rightarrow$ symmetric!!
	- Most existing multicores are SMPs, but not all.
	- **Uniform memory access (UMA):**
		- All processors have a uniform latency from memory
		- Even if the memory is organized into multiple banks
	- **Nonuniform Cache Access (NUCA)**:
		- Some multicores have nonuniform access to the outermost cache, a structure called NUCA for Nonuniform Cache Access, and are thus are **not truly SMPs**, even if they have a single main memory.
		- IBM Power8: Distributed L3 caches with nonuniform access time to different addresses in L3
	- **Multiprocessors consisting of multiple multicore chips?**
		- Because an SMP approach becomes less attractive with a growing number of processors,
		- Often separate memories for each multicore chip $\rightarrow$ Some form of **distributed memory**
2. **Distributed shared memory (DSM)** in Figure 5.2
	- To support larger processor counts
	- Must be distributed among the processors
	- The larger number of processors also raises the need for a high-bandwidth interconnect (See Appendix F)
		- Directed networks (i.e. switches)
		- Indirect networks (eg. multidimensional meshes)
	- Distributing the memory among the nodes both **increases the bandwidth** and **reduces the latency** to **local memory**
	- **Nonuniform memory access (NUMA):**
		- The access time depends on the location of a data word in memory
	- **Key disadvantages**:
		- Communicating **data among processors** becomes somewhat more **complex**
		- DSM requires more effort in the **software**

![[Pasted image 20230525185707.png]]
![[Pasted image 20230525185723.png]]

**Shared memory**:
- In both **SMP and DSM** architectures,
- The **address space is shared** by all the processors!
- Communication among threads occurs through a **shared address space**
- *i.e.* A memory reference can be made by any processor to any memory location, 
	- Assuming it has the correct access rights

**How about larger scale architectures?**
- Clusters and warehouse-scale computers
- Look like individual computers connected by a network
- Memory of one processors cannot be accessed by another processor 
	- Without the assistance of software protocols running on both processors
- So, need message-passing protocols!!

---

## Challenges of Parallel Processing

To be continued ...























---

# Reference

- Chapter 5 in [Computer Architecture A Quantitative Approach (6th)](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1) by Hennessy and Patterson (2017)
- Notebook: [[Computer Architecture Quantitive Approach]]
- [[UMA and NUMA]]
- [[Apple M1 Chip]]