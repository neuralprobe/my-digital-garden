---
title: Multiprocessors and Thread-Level Parallelism
date: 2023-05-29
tags: ComputerArchitecture ThreadLevelParallelism Multiprocessing HennessyPatterson CacheCoherence
---

# Multiprocessors and Thread-Level Parallelism

- Chapter 5 in [Computer Architecture A Quantitative Approach (6th)](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1) by Hennessy and Patterson (2017)

![[Pasted image 20230528112050.png]]
<figcaption> A cute illustration of a distributed shared memory system created by DALL-E</figcaption>

---

# Introduction

The **increased importance of multiprocessing** reflects several major factors:
- Inefficiency of adding more instruction-level parallelism?
	- $\rightarrow$ **Multiprocessing** = Only scalable and general-purpose way 
- A growing interest in **high-end servers** as **cloud computing** and s**oftware-as-a-service**
- A growth in **data-intensive** applications
- Increasing performance on the **desktop is less important** 
	- Highly compute- and data-intensive applications are being done on the **cloud**
- An improved **understanding** of how to use multiprocessors effectively
- The advantages of leveraging a design **investment by replication** rather than unique design

---

In this chapter, we focus on exploiting **thread-level parallelism (TLP)**. **TLP** implies the existence of **multiple program counters** and thus is exploited primarily through **MIMDs**. 

Our focus in this chapter is on **multiprocessors**, which we define as **computers consisting of tightly coupled processors**: 
- Whose coordination and usage are typically controlled by a **single operating system** and 
- That **share memory through a shared address space**.

Such systems exploit thread-level parallelism through **two different software models**:
1. The execution of a **tightly coupled set of threads** collaborating on a **single task**, which is typically called **parallel processing**
2. The execution of multiple, relatively **independent processes** that may originate from one or more users, which is a form of **request-level parallelism** (See Chapter 6 [[Warehouse-Scale Computer]])
	- Maybe exploited **by a single application** running on multiple processors, such as a **database responding to queries**, 
	- Or multiple applications running independently, often called **multiprogramming**.

**Multithreading?** 
- A technique that supports multiple threads executing
- In an interleaved fashion on a single multiple-issue processor
- Many multicore processors also include support for multithreading

Our focus will be on multiprocessors with **roughly 4–256 processor cores**, which might occupy anywhere from **4 to 16 separate chips**;
- **Multicore**?: Multiprocessors in a single chip

**Larger scale computers?**
- See Chapter 6 [[Warehouse-Scale Computer]]
- **Clusters**: Large-scale systems for cloud computing 
- **Warehouse-scale computers**: Extremely large clusters
- **Multicomputer**: 
	- Less tightly coupled than the multiprocessors
	- More tightly coupled than warehouse-scale systems
	- Primary use for high-end scientific computation

---

## Multiprocessor Architecture: Issues and Approach

To take advantage of an **MIMD multiprocessor with $n$ processors**, we must usually have **at least $n$ threads or processes** to execute; 
- With **multithreading**, that **number is 2–4 times higher**.

The **independent threads** within a single process are:
- Identified by the **programmer** or
- Created by the **operating system** (from multiple independent requests) or
- Generated by a **parallel compiler** exploiting data parallelism in the loop

**Grain size:**
- The amount of computation assigned to a thread

**Thread for data-level parallelism**?
- Possible, but, with higher overhead than SIMD processor or GPU due to its small grain size

**Two classes** of shared-memory multiprocessors (**SMP vs DSM architectures** ):
1. **Symmetric (shared-memory) multiprocessors (SMPs)** in Figure 5.1
	- Centralized shared-memory multiprocessors
	- Small to moderate number of cores $\leq 32$
	- The processors share a single centralized memory
	- All processors have equal access $\rightarrow$ symmetric!!
	- Most existing multicores are SMPs, but not all.
	- **Uniform memory access (UMA):**
		- All processors have a uniform latency from memory
		- Even if the memory is organized into multiple banks
	- **Nonuniform Cache Access (NUCA)**:
		- Some multicores have nonuniform access to the outermost cache, a structure called NUCA for Nonuniform Cache Access, and are thus are **not truly SMPs**, even if they have a single main memory.
		- IBM Power8: Distributed L3 caches with nonuniform access time to different addresses in L3
	- **Multiprocessors consisting of multiple multicore chips?**
		- Because an SMP approach becomes less attractive with a growing number of processors,
		- Often separate memories for each multicore chip $\rightarrow$ Some form of **distributed memory**
2. **Distributed shared memory (DSM)** in Figure 5.2
	- To support larger processor counts
	- Must be distributed among the processors
	- The larger number of processors also raises the need for a high-bandwidth interconnect (See Appendix F)
		- Directed networks (i.e. switches)
		- Indirect networks (eg. multidimensional meshes)
	- Distributing the memory among the nodes both **increases the bandwidth** and **reduces the latency** to **local memory**
	- **Nonuniform memory access (NUMA):**
		- The access time depends on the location of a data word in memory
	- **Key disadvantages**:
		- Communicating **data among processors** becomes somewhat more **complex**
		- DSM requires more effort in the **software**

![[Pasted image 20230525185707.png]]
![[Pasted image 20230525185723.png]]

**Shared memory**:
- In both **SMP and DSM** architectures,
- The **address space is shared** by all the processors!
- Communication among threads occurs through a **shared address space**
- *i.e.* A memory reference can be made by any processor to any memory location, 
	- Assuming it has the correct access rights

**How about larger scale architectures?**
- Clusters and warehouse-scale computers
- Look like individual computers connected by a network
- Memory of one processors cannot be accessed by another processor 
	- Without the assistance of software protocols running on both processors
- So, need message-passing protocols!!

---

## Challenges of Parallel Processing

The **first hurdle**: **Insufficient parallelism**

**Amdahl's Law**:
- To achieve a speedup of 80 with 100 processors,
- Only 0.25% of the original computation can be sequential!
![[Pasted image 20230526033202.png]]

The **second major challenge**: **Large latency of remote access** in a parallel processor;
- Communication latency 
	- Between separate cores: **35–50** clock cycles 
	- Among cores on separate chips: **100 ~ More than 300** clock cycles
	- Depending on the **communication mechanism**, the type of **interconnection** network, and the **scale** of the multiprocessor.
- Example
	- Multiprocessor with 4GHz clock rate
	- 100ns delay to handle a reference to a remote memory
	- All local access hit cache
	- Base CPI = 0.5
	- CPI with 0.2% remote communication reference?
		- 0.5 +0.2% $\times$ Remote request cost = 1.3
	- $\rightarrow$ **Multiprocessor with all local references is 1.3/0.5 = 2.6 times faster**

---

# Centralized Shared-Memory Architectures

The observation that the use of **large, multilevel caches** can substantially **reduce the memory bandwidth demands** of a processor is the key insight that **motivates centralized memory multiprocessors**. 

**Access to memory is asymmetric** in multiprocessors: 
- Faster to the local memory 
- Slower to the remote memory

**Symmetric shared-memory machines** support the caching of both ...
- **Private** data used by a single processor
	- Cached like a uniprocessor
- **Shared** data used by multiple processors
	- Cache coherence problem!

---

## What Is Multiprocessor Cache Coherence?

**Cache coherence problem:**
- Because the view of memory held by **two different processors** is through **their individual caches**, 
- The processors could end up **seeing different values for the same memory location**!
- *i.e.* **Local** states in private caches vs **Global** state in shared cache or main memory

![[Pasted image 20230526044252.png]]

Two different aspects of memory system behavior:
- **Coherence**
	- Defines **what** values can be returned by a read
- **Consistency**
	- Determines **when** a written value will be returned by a read

**A memory system is coherent if:** 
(P = Processor, P' = Another processor, X = Memory location)
1. **Preserve program order:**
	- Write by P to X $\rightarrow$ Read by P to X $\rightarrow$ Always returns the value written by P
	- If no writes of X by P' when P writes and reads
	- True in even uniprocessors
2. **Have a coherent view of memory:**
	- Write by P' to X $\rightarrow$ Read by P to X $\rightarrow$ Returns the written value by P'
		- If the read and write are sufficiently separated in time and no other writes to X occur between the two accesses
3. **Write serialization:**
	- Writes to the same location are **serialized**
	- Two writes to the same location by any two processors are seen in the **same order by all processors**.
	- For example, if the values 1 and then 2 are written to a location, processors can never read the value of the location as 2 and then later read it as 1.

**Memory consistency model:** 
- The question of **when** a written value will be seen is also important.
- Observe that 
	- We cannot require that a **read of X** **instantaneously see** the value written for X by some other processor. 
- For example, 
	- If a write of X on one processor precedes a read of X on another processor by a very small time, 
	- It may be impossible to ensure that the read returns the value of the data written, 
	- Since the written data may not even have left the processor at that point.

**Coherence and consistency are complementary:** 
- Coherence defines the behavior of reads and writes **to the same memory location**
- Consistency defines the behavior of reads and writes **with respect to accesses to other memory locations**

**Assume from now** until Section 5.6 (**Memory Consistency Model**)
- First, a write does not complete (and allow the next write to occur) until all processors have seen the effect of that write.
- Second, the processor does not change the order of any write with respect to any other memory access. 
- These two conditions mean that, if a processor **writes** location **A** followed by location **B**, any processor that sees the **new value of B** must **also see the new value of A**

---

## Basic Schemes for Enforcing Coherence

A program running on **multiple processors** will normally have **copies** of the same data in several **caches**. In a coherent multiprocessor, the caches provide both 
- **Migration** 
	- A data item can be moved to a local cache and used there in a transparent fashion
	- Reduces both the **latency** to access a shared data item that is allocated remotely and the **bandwidth demand** on the shared memory
- **Replication** of shared data items
	- Reduces both **latency** of access and **contention** for a read shared data item

**Cache coherence protocols:**
- Multiprocessors adopt a **hardware solution** by introducing a **protocol** to maintain coherent caches, that supports migration and replication.
- Key idea = "**Tracking the state of any sharing of a data block**"
- Two classes of protocol in use
	- Directory based
	- Snooping

**Directory based**:
- The sharing status of a particular block of physical memory is kept in one location, called the **directory**
- **Two very different types** of directory-based cache coherence
	- (1) **One centralized directory** in an **SMP**
	- (2) **Distributed directories** in a **DSM** (See Section 5.4)

**Snooping:**
- **Each cache track**s sharing status of memory blocks they store.
- In **SMP**, the caches are typically all accessible via some **broadcast** medium (a **bus** connects the per-core caches to the shared cache or memory)
- All **cache controllers** monitor or **snoop** on the medium to determine whether they have a copy of a block that is requested on a bus or switch access.
- **Snooping** can also be used as **the coherence protocol** for a **multichip multiprocessor**, and some designs support a 
	- **Snooping** protocol on top of a **directory** protocol within each multicore.

---

## Snooping Coherence Protocols

**Two ways** to maintain the coherence requirement:
- Write invalidate protocol
- Write update or write broadcast protocol

**Write invalidate protocol**:
- Ensure that a processor has **exclusive access** to a data item before writing that item
- It **invalidates** other copies on a write
- The most common protocol
- For a write, 
	- We require that the writing processor has **exclusive access**, 
	- **Preventing** any other processor from being able to write simultaneously
- **Write serialization**
	- If **two** processors do attempt to write the same data simultaneously, one of them **wins the race**
		- Causing the other processor’s copy to be invalidated
	- For **the other processor** to complete its write, 
		- It must **obtain a new copy of the data**, which must now contain the updated value.

![[Pasted image 20230527024253.png]]

**Write update or write broadcast protocol:**
- Must broadcast all writes 
- Consumes considerably more bandwidth
- Virtually all recent multiprocessors have opted to implement a write invalidate protocol

---

## Basic Implementation Techniques

**Implementing an invalidate protocol?**
- Use of the bus to perform invalidate 
	- Or another broadcast medium
- In **older** multiple-chip multiprocessors, 
	- The bus used for coherence is the **shared-memory access bus** 
- In a single-chip multicore, 
	- The bus can be the **connection between the private caches (L1 and L2 in the Intel i7) and the shared outer cache (L3 in the i7)**. 
- To perform an **invalidate**, 
	- The processor simply **(1) acquires bus access** and **(2) broadcasts the address** to be invalidated on the bus.
	- All **processors** continuously **(3) snoop** on the bus, watching the addresses. 
	- The processors **(4) check whether the address on the bus is in their cache**. If so, the corresponding data in the cache are **(5) invalidated**.

**Two processors attempt to write shared blocks at the same time?**
- Do write serialization!
- The first processor to obtain bus access will cause 
	- Any other copies of the block it is writing to be invalidated
- The core with the sole copy of a cache block is normally called the **owner** of the cache block

**Things to implement:**
- How to **obtain bus access**
- How to enforce **write serialization**
- How to **invalidate** outstanding copies of a cache block that is being written into
	- Use a **valid bit** of each cache block 
- How to **locate a data item** when a cache miss occurs in a write-back cache
	- For a write-through cache, simply fetch the most recent value from the memory
	- For a write-back cache, 
		- The most recent value of a data can be in a **private cache**!
		- Use the **same snooping** scheme both for **cache misses** and for **writes**
		- **Cache misse steps:**
			- **(1) A cache miss** occurred in a requestor *P*
			- The owner *P'* (**2) snoops the read request** from the requestor *P* to the memory (or L3)
			- If the owner *P'* has updated the request cache block, and it has **(3) a dirty bit**
			- Then, the owner P' ...
				- **(4) Provides the cache block** in response the read request
					- If the owner P' owns the cache block exclusively
					- Change the state of the cache block **(5) from exclusive (unshared) to shared**
				- Causes the memory (or L3) **(6) access to be aborted**
			- The requestor **(7) gets the copy** of the cache block
		- **Write steps:**
			- Q. Any other copies of the block are **(1) cached?**
				- Check a bit indicating whether the block is shared
			- **No** $\rightarrow$ The write does **not need** to be placed on the bus in a write-back cache
			- **Yes** $\rightarrow$ 
				- **(2) Generate an invalidate** on the bus
				- Marks the block as **(3) exclusive** (unshared)

**Every bus transaction must check the cache-address tags**, which could potentially **interfere with processor cache accesses**. How to reduced the interference? :
- (Approach\#1) **Duplicate the tags** (just to allow checks in parallel with CPU) and have snoop accesses directed to the duplicate tags
- (Approach\#2) Use a **directory** at the shared L3 cache 
	- Then, **invalidates** can be **directed only to** those caches with copies of the cache block. 
	- This requires that L3 must always have a copy of any data item in L1 or L2, a property called **inclusion**

---

## An Example Protocol

**Finite-state controller** in each core:
- Responds to requests **from the processor** in the core and **from the bus** 
- **Changes the state** of the selected cache block
- Use the bus to 
	- **Access** data
	- **Invalidate** the data

The **simple protocol** we consider has three states: 
- **invalid**
- **shared**: Indicates that the block in the private cache is potentially **shared**,
- **modified**: Indicates that the block has been **updated** in the private cache, so **exclusive**

![[Pasted image 20230527051516.png]]

A **finite-state transition diagram** for a single private cache block using a write invalidation protocol and a write-back cache:

![[Pasted image 20230527053551.png]]
![[Pasted image 20230527053559.png]]

![[Pasted image 20230527054017.png]]

The most **important assumption** so far is that the protocol assumes that **operations are atomic**—that is, an operation can be done in such a way that **no intervening operation** can occur. 
- For example, the protocol described assumes that write misses can be detected, acquire the bus, and receive a response as **a single atomic action.** 
- **In reality this is not true. In fact, even a read miss might not be atomic**; 
	- After detecting a miss in the L2 of a multicore, 
	- The core must arbitrate for access to the bus connecting to the shared L3. 
	- **Nonatomic actions** introduce the possibility that the protocol can **deadlock**, 
	- Meaning that it reaches a state where **it cannot continue**.

**Hardware implementation:**
- **Single-chip** multicore processors 
	- $\rightarrow$ **snooping** or **simple central directory protocol**
- **Many** multiprocessor **chips**
	- eg. Intel Xeon, AMD Opteron with high-speed interface
	- Usually have a distributed memory architecture and 
	- $\rightarrow$ Inter-chip coherency mechanism
	- $\rightarrow$ **Directory scheme**

---

## Extensions to the Basic Coherence Protocol

**Basic protocol described so far:** 
- **MSI** protocol 
	- Uses only three states such as `modified`, `shared`, and `invalid` (That why it's called MSI)
- **Many extensions** of this basic protocol
	- Adding additional states and transactions
	- (1) **MESI**: Modified, Exclusive, Shared, and Invalid
	- (2) **MOESI**: Modified, Owned, Exclusive, Shared, and Invalid

**Two of the most common extensions:**
- **MESI**
	- Four states: Modified, **Exclusive**, Shared, and Invalid
	- **Exclusive** state indicates that a cache block is resident in **only a single cache** **but is clean** 
	- In MSI protocol, this case was included the `Shared` state
	- A block is in the **E** state
		- It can be **written without**
			- **Acquiring bus access**
			- **Generating any invalidates**
		- Which optimizes the case where a block is read by a single cache before being written by that same cache.
	- **MESIF**
		- A variant of MESI in Intel i7
		- Adds a **Forward** state
		- Designate **which** sharing processor **should respond** to a request
		- Designed to enhance performance in distributed memory organizations
- **MOESI**
	- Five states: Modified, **Owned**, Exclusive, Shared, and Invalid
	- **Owned state** : indicate that the associated block is owned by that cache and out-of-date in memory.
	- In MSI and MESI, 
		- Attempt to share a block in the Modified state
		- $\rightarrow$ Changed to Shared (in both the original and newly sharing cache)
		- The block must be written back to memory
	- In a MOESI protocol, 
		- Attempt to share a block in the Modified state
		- $\rightarrow$ Changed to Owned state in the original cache 
			- Without writing it to memory
		- Other caches with the data supplied from the owner $\rightarrow$ Shared state
	- **AMD Opteron**

---

## Limitations in Symmetric Shared-Memory Multiprocessors and Snooping Protocols

**Increasing communication burden:**
- \#processors in multiprocessor $\uparrow$ , 
- Memory demands of each processor $\uparrow$, 

$\rightarrow$ **Any centralized resource can be bottleneck:** 
- **Single shared bus**
- **Snooping bandwidth at the caches**

**Three different approaches**:
- IBM Power8
	- **8 parallel buses** connect
		- 12 processors in a single multicore
		- Distributed L3 and
		- Up to 8 separate memory channels
	- Nonuniform access time for both L3 and memory
- Xeon E7
	- **Three rings** connect
		- Up to 32 processors
		- Distributed L3 cache
		- Two/Four memory channels
	- Not uniform, but
	- Can operate as if access times were uniform
- Fujitsu SPARC64X+ 
	- **A crossbar** to connect
		- A shared L2 
		- To up to 16 cores and multiple memory channels
	- Symmetric organization with uniform access time

- Example 
	- Assumption:
		- 8-processors where each processor has its own L1 and L2
		- Snooping on a shared bus among L2s
		- Average L2 request = 15 cycles
		- Clock rate = 3.0 GHz, CPI = 0.7
		- load/store frequency = 40%
	- Question
		- If our goal is that no more than 50% of the L2 bandwidth is consumed by coherence traffic, what is the maximum coherence miss rate per processor?

![[Pasted image 20230528052821.png]]

Several techniques for **increasing the snoop bandwidth:**
- (1) Tags can be **duplicated** just to allow checks in parallel with CPU
	- Doubles the effective cache-level snoop bandwidth
	- Average cost of a CMR(coherence miss rate) decrease to 12.5cycles 
		- If 50% the coherence requests do not hit with 10 cycle snoop request cost
- (2) **Each processor** with **distributed** memory **handles** snoops individually and **broadcast** to L2 again for **snoop hit**
	- Each processor has a portion of the memory
	- Handles snoops for that portion of the address space
	- NUCA design used by IBM 12-core Power8
		- But effectively scales the snoop bandwidth at L3 by the \#processors. 
		- Detail steps: If there is a **snoop hit** in L3, then we must still **broadcast to all L2** caches, which must in turn snoop their contents. 
		- Since L3 is acting as a filter on the snoop requests, L3 must be inclusive
	- Still need broadcast for snoop hit!!
- (3) Place a **directory** at the level of the outermost shared cache (eg. L3)
	- L3 acts as a filter on snoop requests and must be inclusive
	- We need not snoop or broadcast to all the L2s. 
	- Both L3 and the associated directory entries can be distributed
	- Intel Xeon E7 series: 8~32 cores

![[Pasted image 20230528054506.png]]

- **Intermediate point** between a snooping and a directory protocol
	- Used in **AMD Opteron**
		- Memory is directly connected to each multicore chip (Up to four multicore chips)
		- NUMA: Nonuniform memory access
			- Local memory is somewhat faster
	- Opteron's coherence protocol
		- Point-to-point links (Not shared) to broadcast up to three other chips
		- Not shared $\rightarrow$ Need explicit acknowledgment to know that invalidation operation has completed 
		- Uses a broadcast to find potentially shared copies, 
			- like a snooping protocol, 
		- But uses the acknowledgments to order operations, 
			- like a directory protocol

**Directory**-based protocols, which **eliminate the need for broadcast** to all caches on a miss? **See Section 5.4**
- Directories within the multicore (Intel Xeon E7) or
- Add directories when scaling beyond a multicore

---

## Implementing Snooping Cache Coherence

The major complication in actually implementing the **snooping coherence protocol** we have described is that write and upgrade misses are **not atomic in any recent multiprocessor**. 

- In a **multicore with a single bus**, 
	- these steps can be **made effectively atomic** 
	- by arbitrating for the bus to the **shared cache or memory first** (before changing the cache state) and 
	- not releasing the bus until all actions are complete. 
- **Without a single, central bus**, 
	- we must find some other method of making the steps in a miss **atomic**. 
	- In particular, we must ensure that two processors that attempt to write the same block at the same time, a situation which is called a **race**, are **strictly ordered**: 
		- one write is processed and precedes before the next is begun.
	- **In a multicore using multiple buses**, 
		- races can be eliminated if each block of memory is associated with only a single bus,
		- ensuring that two attempts to access the same block must be serialized by that common bus. 
		- This property, together with the ability to **restart the miss handling of the loser in a race**, are the keys to implementing snooping cache coherence without a bus. We explain the details in Appendix I.

- Possible to **combine snooping and directories**
	- several designs use snooping within a multicore and directories among multiple chips
	- A combination of directories at one cache level and snooping at another level

---

## Performance of Symmetric Shared-Memory Multiprocessors

Overall cache performance is a combination of the behavior of: 
- Uniprocessor cache miss traffic by **cache miss rate**
	- Capacity
	- Compulsory
	- Conflict
- traffic caused by **coherence miss rate**
	- **True sharing misses**
		- The first write by a processor to a shared cache block causes an invalidation to establish ownership of that block
	- **False sharing**
		- Occurs when a **block** is **invalidated** (and a subsequent reference causes a miss) because some **word** in the block, **other than the one being read, is written into.**
		- The **block is shared**, but **no word** in the cache is actually **shared**
		- **Block size?**
			- The miss would not occur if the block size were a single word

- Example of False sharing
	- Words `z1` and `z2` are in the same cache block
	- The block is shared by `P1` and `P2` processors

![[Pasted image 20230528133830.png]]

---

Will upload workload analysis subsections such as ...
- A Commercial Workload
- A Multiprogramming and OS Workload
- Performance of the Multiprogramming and OS Workload

---

# Distributed Shared-Memory and Directory-Based Coherence

The **absence of any centralized data structure** that tracks the state of the caches is both the fundamental **advantage** of a snooping-based scheme, since it allows it to be **inexpensive**, as well as its **Achilles’ heel** when it comes to **scalability**.

**Directory protocol:**
- **Keeps the state of every block** that may be cached
- Information in the directory includes 
	- **which caches** (or collections of caches) have copies of the block, 
	- whether it is **dirty**, and so on. 
- **Single directory** within **a multicore with a shared outermost cache**
	- Say, L3
	- Easy to implement a directory scheme
	- Simply **keep a bit vector** of the size **equal to the number of cores** for each L3 block
	- **No broadcast**
		- **Invalidations** are sent **only to the caches** that shares the same cache blocks
	- Still, not scalable!! 
		- Even though it avoids broadcast
- **Distributed directory**
	- Must **know where to find** the directory information for any cached block of memory
	- **Distribute the directory along with the memory** so that different coherence requests can go to different directories, just as different memory requests go to different memories.
	- **Avoid broadcast!**
		- The sharing status of a block is stored in a single known location
	- Directory in a shared outer cache (L3)?
		- Distribute the directory information to different cache banks!
	- Simplest implementation?
		- Associate an entry in the directory with each memory  block
		- Node can be 
			- a single multiprocessor
			- a small collection of processors that implements coherence internally
		- \#information $\propto$ \#(memory blocks)$\times$\#nodes

![[Pasted image 20230528160155.png]]

---

## Directory-Based Cache Coherence Protocols: The Basics

**Two primary operation that a directory protocol must implement:**
- (1) Handling a **read miss** and
- (2) Handling a **write** to a shared, clean cache block.
- (1+2) Handling a **write miss** to a block that is currently shared is a simple combination of these two.

**State of each cache block:**
- **Shared** — One or more nodes have the block cached, and the value in memory is up to date (as well as in all the caches)
- **Uncached** — No node has a copy of the cache block.
- **Modified** — Exactly **one node** has a copy of the cache block, and it has written the block, so the memory copy is out of date. The processor is called the **owner** of the block

**Must track:**
- the **state** of each potentially shared memory block and
- **which nodes** have copies of that block because those copies will need to be invalidated on a write

**Implemented by keeping a bit vector for each memory block**:
- Each bit of the vector indicates whether the **corresponding processor chip** (which is likely a multicore) has a **copy of that block**
- \+ Keep track of the **owner** of the block when the block is in the **exclusive** state
- + For efficiency reasons, we also track the **state** of each cache block at the **individual caches**

**A catalog of the message types** that may be sent between the **processors and the directories** for the purpose of handling misses and maintaining coherence
- **Local node** $-$ the node where a **request** originates. 
- **Home node** $-$ the node where the **memory location** and the **directory entr**y of an address reside
	- The location of the home node is known for a given physical address
- **Remote node** $-$ the node that has a copy of a cache block, whether **exclusive** (in which case it is the only copy) or **shared**

![[Pasted image 20230529040523.png]]

In this section, we **assume a simple model of memory consistency**. 
- Messages will be 
	- **Received and acted upon** **in the same order they are sent**.
	- To minimize the type of messages and the complexity of the protocol, 
- Ensure that **invalidates sent by a node** are **honored before new messages are transmitted**
- **Not true in practice**, See **Section 5.6**

---

## An Example Directory Protocol

**(The 1st half implementation of the directory-based coherence) State transition diagram for an individual cache block:**
(Gray \= Requests coming from outside the node, Bold \= Actions)
- Almost identical with snooping case
- Write miss operation 
	- Snooping-based $\rightarrow$ Broadcast
	- Directory-based $\rightarrow$ Data fetch and invalidate
![[Pasted image 20230529043846.png]]

**(The 2nd half implementation of the directory-based coherence)  State transition diagram for the directory:**
- A message **sent to a directory** causes **two different types of actions**
	- **Updating** the directory state and 
	- **Sending** additional messages to satisfy the request
- **Directory states:**
	- Unlike in a snooping scheme, however, the **directory state** indicates **the state of all the cached copies of a memory block**, rather than for a single cache block.	
	- Memory block may be
		- **Uncached** by any node, 
		- Cached in multiple nodes and readable (**shared**), or 
		- Cached **exclusively** and writable in exactly one node
	- **Sharers**
		- A set to perform a function that tracks the set of nodes that have a copy of a block
- A directory receives **three different requests**
	- **Read miss**
		- Uncached $\rightarrow$ Shared
			- The requesting node is sent the requested data from memory and become the only sharing node
		- Shared $\rightarrow$ Shared
			- The requesting node is sent the requested data from memory and become a sharing node
		- Exclusive $\rightarrow$ Shared
			- The owner is sent a data fetch message, send data to the directory (in memory), and become sharing node
			- The requesting node is sent the requested data from the memory and become a sharing node
	- **Write miss**
		- Uncached $\rightarrow$ Exclusive
			- The requesting node is sent the value and become the only sharing node
		- Shared $\rightarrow$ Exclusive
			- The requesting node is sent the value and become the only sharing node
			- All nodes in the set Sharers are sent invalidate messages
		- Exclusive $\rightarrow$ Exclusive
			- The block has a new owner
			- The old owner is invalidated, send the value to the directory, and finally to the requesting node 
			- Maybe optimized by directly forwarding the value from the old owner to the new owner 
	- **Data write-back**
		- Exclusive $\rightarrow$ Uncached
			- The owner is replacing the block and therefore must write it back. 


![[Pasted image 20230529050219.png]]

**Note**: All actions are assume to be **atomic**, which is not true in reality. Check Appendix I that explore **non-atomic** memory transactions

**Additional optimization?**
- Write-miss to an exclusive block:
	- Many of the protocols in use in commercial multiprocessors **forward the data from the owner node to the requesting node directly** (as well as performing the write-back to the home).
	- Side effect: 
		- Add complexity by increasing the possibility of deadlock and by increasing the types of messages

---

# Synchronization: The Basics

To be continued!








---

# Reference

- Chapter 5 in [Computer Architecture A Quantitative Approach (6th)](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1) by Hennessy and Patterson (2017)
- Notebook: [[Computer Architecture Quantitive Approach]]
- [U.C.Berkeley lecture slide(CS252) on snooping vs directory based coherency by D. A. Patterson](https://people.eecs.berkeley.edu/~pattrsn/252F96/Lecture18.pdf)
- [[UMA and NUMA]]
- [[Apple M1 Chip]]

